{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76ExHElN5dy5"
   },
   "source": [
    "# Deep Autoencoder applied to the stroke dataset\n",
    "---\n",
    "\n",
    "\n",
    "In this notebook, an *autoencoder* is implemented using `Keras` consisted of only one Dense Layer with a Nonlinear activation function such as the `ReLU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%cd \"Notebook utilities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9gsZaXWq5l6A"
   },
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random \n",
    "import tqdm \n",
    "import gc\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import time\n",
    "\n",
    "import os \n",
    "from os.path import dirname, join as pjoin\n",
    "from Data_Preprocessing import *\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vpj7BF1IIGE_"
   },
   "outputs": [],
   "source": [
    "#Create dataset\n",
    "#get data\n",
    "mat_path = '/content/drive/MyDrive/INTERN THESIS/FC_Stroke/FCMatrixImage_131subj.mat'\n",
    "lang_path = '/content/drive/MyDrive/INTERN THESIS/FC_Stroke/language_score.xlsx'\n",
    "Normalize = True\n",
    "\n",
    "fc_3d, language_score, ID = get_arrays(mat_path, lang_path, Normalize)\n",
    "\n",
    "#vectorizing matrices\n",
    "vect_mat = vectorize_data(fc_3d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_g14gTLXEj86",
    "outputId": "229cc1c1-4e10-4656-bb87-5900b27f60c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:00<00:00, 186.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "FC = sio.loadmat(mat_path)\n",
    "n_subj = len(FC['img'][0])\n",
    "len_data = len(FC['img'][0][0][1])\n",
    "\n",
    "fc_3d = np.zeros((n_subj,len_data, len_data) )\n",
    "\n",
    "from tqdm import tqdm\n",
    "for subject in tqdm(range(n_subj)):\n",
    "    \n",
    "    if len(FC['img'][0][subject][1]) != 0:\n",
    "        if np.all(FC['img'][0][subject][1] != 0):\n",
    "            df = pd.DataFrame(FC['img'][0][subject][1])\n",
    "            #converting NA to 0 values - The one in the diagonal are NA\n",
    "            df = df.fillna(0)\n",
    "            \n",
    "            sc.fit(df)\n",
    "            df = sc.transform(df)\n",
    "\n",
    "            #df = (df - df.mean())/df.std()\n",
    "            fc_3d[subject] = np.asanyarray(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8f-9S-TBqOTt"
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "indexes = []\n",
    "for i in fc_3d:\n",
    "    if np.all(i == 0):\n",
    "        indexes.append(idx)\n",
    "    idx+=1\n",
    "fc_3d = np.delete(fc_3d, indexes, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AJGCU-yYqOLj"
   },
   "outputs": [],
   "source": [
    "#vectorizing matrices\n",
    "vect_mat = vectorize_data(fc_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cErdswtjN2Yx"
   },
   "outputs": [],
   "source": [
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "components = np.arange(10, 100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hBcXHY2vH_s_",
    "outputId": "21ca3d83-bde8-4e1f-be5b-a1f89e5e27cb"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow import keras\n",
    "\n",
    "inp_shape = vect_mat.shape[1]\n",
    "input_img = Input(shape = (inp_shape,))\n",
    "mse = np.zeros(components.shape[0])\n",
    "se = np.zeros(components.shape[0])\n",
    "embedded_space = {}\n",
    "weigth = {}\n",
    "timing = []\n",
    "for i, encoding_dim in enumerate(tqdm(components)):\n",
    "  name = 'n{}'.format(encoding_dim)\n",
    "\n",
    "  start_time = time.time()\n",
    "  encoded = Dense(encoding_dim, activation='LeakyReLU')(input_img)\n",
    "  decoded = Dense(inp_shape, activation='LeakyReLU')(encoded)\n",
    "  # this model maps an input to its reconstruction\n",
    "  autoencoder = Model(input_img, decoded)\n",
    "  #Encoder\n",
    "  encoder = Model(input_img, encoded)\n",
    "  # create a placeholder for an encoded (32-dimensional) input\n",
    "  encoded_input = Input(shape=(encoding_dim,))\n",
    "  # retrieve the last layer of the autoencoder model\n",
    "  decoder_layer = autoencoder.layers[-1]\n",
    "  # create the decoder model\n",
    "  decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "  opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "  autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "  history = autoencoder.fit(vect_mat,vect_mat,\n",
    "                  epochs=50,\n",
    "                  batch_size=16,\n",
    "                  validation_split=0.1,\n",
    "                  shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "  res = autoencoder.predict(vect_mat)\n",
    "  encoded_imgs = encoder.predict(vect_mat)\n",
    "  decoded_imgs = decoder.predict(encoded_imgs)\n",
    "  embedded_space[name] = encoded_imgs\n",
    "  rer = np.mean((vect_mat - decoded_imgs)**2, 1)\n",
    "  mse[i] =np.mean(rer)/np.sqrt(324)\n",
    "  se[i] = np.std(rer, ddof=1) / np.sqrt(len(rer))\n",
    "  weigth[name] = autoencoder.weights\n",
    "  end_time = time.time()\n",
    "  timing.append(end_time - start_time)\n",
    "  print(mse)\n",
    "\n",
    "  #plot  losses \n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('model train vs validation loss')\n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'validation'], loc='upper right')\n",
    "  plt.savefig('Non_Losses'+str(encoding_dim))\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "  n=10\n",
    "  plt.figure(figsize=(20, 4))\n",
    "  for i in range(n):\n",
    "      # display original\n",
    "      ax = plt.subplot(2, n, i + 1)\n",
    "      plt.imshow(from_vec_to_mat(vect_mat[i], 324), cmap = 'jet')\n",
    "      plt.title(\"original\")\n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)\n",
    "\n",
    "      # display reconstruction\n",
    "      ax = plt.subplot(2, n, i + 1 + n)\n",
    "      plt.imshow(from_vec_to_mat(decoded_imgs[i], 324), cmap = 'jet')\n",
    "      plt.title(\"reconstructed\")\n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)\n",
    "  plt.savefig('Non_Linear_Recostructed_Images_'+str(encoding_dim))\n",
    "  plt.show()\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7VidcbcHIz6E"
   },
   "outputs": [],
   "source": [
    "np.savetxt('MSE_NONLINEARAE.txt', mse)\n",
    "np.savetxt('SD_NONLINEARAE.txt', se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wi3AS95FPXo_"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "a_file = open(\"FEATURES_NONLINEARAE.pkl\", \"wb\")\n",
    "pickle.dump(embedded_space, a_file)\n",
    "a_file.close()\n",
    "\n",
    "a_file = open(\"WEIGHTS_NONLINEARAE.pkl\", \"wb\")\n",
    "pickle.dump(weigth, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szcv5HR4ykio"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(histogram_rer)\n",
    "data.index = ['10', '50', '90']\n",
    "import seaborn as sns\n",
    "sns.set(font_scale = 2)\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.kdeplot(data=data.T, fill=True, common_norm=False, palette=\"Accent\",\n",
    "   alpha=.5, linewidth=2)\n",
    "plt.savefig('Distribution_nonLinear')\n",
    "data.to_csv('nonlinear_distribution.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NONLINEAR_AUTOENCODER_STROKE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
